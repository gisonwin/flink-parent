#### 梯度下降法(Gradient Descent)

from 百度百科

梯度下降是迭代法的一种,可以用于求解最小二乘问题(线性和非线性).在求解机器学习算法的模型参数,即无约束优化问题时,该算法是常采用方法之一.另一种是最小二乘法.

​	在求解损失函数的最小值时,可以通过梯度下降法来一步步的迭代求解,得到最小化的损失比偶不同模型参数值.反之我们求解损失函数的最大值,就需要用梯度上升法来迭代.

​	在机器学习中发展了两种梯度下降方法:随机梯度下降法和批量梯度下降法.

1. 微分

   - 定义
   - 函数图像中,某点的切线的斜率
   - 函数的变化率

2. 多微分

   - 当一个函数有多个变量的时候,就是多变量的微分,即分别对每个变量进行求微分 

3. 梯度

   - 实际上就是多变量微分的一般化.
   - 单变量的函数中,梯度就是函数的微分,代表函数在某个给定点的切线的斜率.
   - 多变量函数中,梯度是一个向量,向量有方向,梯度的方向就指出了函数在给定点的上升最快的方向.

4. 梯度下降法公式

5. 算法实现

   - 定义一个代价函数,均方误差代价函数
     $$
     J(\theta)=\frac{1}{2m}\sum_{x=1}^{m}(h_e(x^{(f)})-y^{x})^2
     $$

------

#### 逻辑回归算法(logistic regression)

1. 逻辑回归与线性回归的关系

   逻辑回归是用来做分类算法的,大家熟悉线性回归,一般形式是
   $$
   y=ax+b,\quad y\in[-\infty,+\infty]
   $$
   首先我们来看一个Sigmoid函数.
   $$
   S(t)=\frac{1}{1+e^{-t}}
   $$
   ​	函数中t无论取什么值,其结果都在[0,-1]的区间内,回想一下,一个分类问题就有两种答案,一种是"是",一种是"否",则0对应着否,1对应着是.但有人问了,这不是[0,1]区间吗,怎么会只有0和1呢?

   ​	假设我们分类的阈值是0.5,则超过0.5的归为1分类,低于0.5的归为0分类,阈值是可以自己设定的.下面我们把ax+b带入t中就得到了我们的逻辑回归的一般模型方程:
   $$
   p(x;a,b)=\frac{1}{1+e^{-ax+b}}\dots\dots(1)
   $$
   结果p也可以理解为概率,就是说概念大于0.5的属于1分类,小于0.5的属于0分类,这就达到了分类的目的.

2. 损失函数

   逻辑回归的损失函数跟其它的不同:
   $$
   Cost(h_{\theta}(x),y)=\begin{cases} -log(h_{\theta}(x))  &if \ y=1 \\ -log(1-h_{\theta}(x))  &if\ y=0 \end{cases}
   $$
   ​	当真实值为1分类时,用第一个方程来表示损失函数;当真实值为0分类时,用第二个方程来表示损失函数,为什么要加上log函数呢?当真实样本为1时,但h=0的概率,则$log0=\infty$,这就对模型最大的惩罚力度;当h=1时,则$log1=0$.相当于没有惩罚力度,也就是没有损失,达到最优结果.所以数学家就想出了用log函数来表示损失函数,把上述两式合并起来就是如下的函数,并加上正则化项
   $$
   \begin{align} 
   J(\theta)&=\frac{1}{m}\sum_{i=1}^{m}Cost(h_{\theta}(x^{(i)},y^{(i)})) \\ 
   &=-\frac{1}{m}[\sum_{i=1}^{m}y^{(i)} log h_{\theta}(x^{(i)} )+(1-y^{(i)})log (1-h_{\theta}(x^{(i)}))]
   \end{align}
   $$
   加上正则化项后
   $$
   J(\theta)=[-\frac{1}{m}\sum_{i=1}^{m}y^{(i)} log h_{\theta}(x^{(i)} )+(1-y^{(i)})log (1-h_{\theta}(x^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}
   $$
   最后按照梯度下降法一样,求解极小值点,得到想要的模型效果.
   
3. 多分类问题(one vs rest)

   我们从二分类问题过渡到多分类问题,步骤如下

   - 将类型class1看作正样本,其他类型全部看作负样本,然后我们就可以得到样本标记类型为该类型的概率p1
   - 然后再将另外类型class2看作正样本,其他类型全部看作负样本,同理得到p2
   - 依次循环,我们可以得到该待测样本的标记类型分别为类型class i时的概率pi,最后我们取pi中最大的那个概率对应的样本标记类型作为我们的待预测样本类型.

   总之还是以二分类来依次划分,并求出概率结果.

4. 逻辑回归的一些经验

   - 模型本身并没有好坏之分
   - LR能以概率的形式输出结果,而非只是0,1判定
   - LR的可解释性强,可控度高
   - 训练快,feature engineering之后效果很好
   - 因为结果是概率,可以做ranking model.

5. 逻辑回归的应用

   - CRT预估/推荐系统的learning to rand/各种分类场景
   - 某搜索引擎的广告CTR预估基线版是LR
   - 某电商搜索排序/广告CTR预估基线版是LR
   - 某电商购物搭配推荐用了大量LR
   - 某新闻app排序基线是LR

------

用户id 订单次数 订单频次 浏览男装  浏览小孩 浏览老人 浏览女士 订单均额  浏览商品频次 标签

------

#### Kmeans算法

1. 定义

    		Kmeans算法是一种聚类算法,所谓聚类算法即根据相似性原则,将具有较高相似度的数据对象划分至同一类簇,将具有较高相异度的数据对象划分至不同类簇.

   ​		聚类与分类最大的区别在于聚类过程为无监督过程,即待处理数据对象没有任何先验知识,而分类过程为有监督过程,即存在有先验知识的训练数据集.

2. 算法原理

   ​		Kmeans算法中k代表类簇个数,means代表类簇内数据对象的平均值(这种均值是一种对类簇中心的描述),因此Kmeans算法又移为k-均值算法.它是一种基于划分的聚类算法,以距离作为数据对象间相似度量的标准,即数据对象间的距离越小,则它们的相似度越高,则它们越有可能在同一个类簇.数据对象间距离的计算有很多种,k-means算法通常采用欧氏距离来计算数据对象间的距离.
   $$
   \begin{align} 
   &----------------------------- \\
   & 输入:类簇个数K,迭代终止阈值\delta  \\ 
   & 输出:聚类结果\\
   &----------------------------- \\ 
   & For \; t=1,2,\dots,T \\
   & \qquad\quad  For\ every \ x_{i}(对于所有数据对象) \\
   &根据公式(1),计算dist(x_{i},Center_{k});\\
   &将x_{i}划分至距离其最近的类簇中心所在类簇中;\\
   & \qquad\quad End \ for \\
   &根据公式(2),更新所有类簇中心;\\
   &根据公式(3),计算两次迭代的差值\Delta{J};\\
   & \qquad\quad if \; \Delta{J} < \delta \\
   & \qquad\qquad\quad Then \ 输出聚类结果;\\
   & \qquad\qquad\quad break;\\ 
   & \qquad\quad End \ if  \\ 
   &End \ for 
   \end{align}
   $$

3. 优缺点分析

   - 优点: 算法简单易实现
   - 缺点:
     - 需要用户事先指定类簇个数;
     - 聚类结果对初始类簇中心的选取较为敏感;
     - 容易陷入局部最优;
     - 只能发现球形类簇;
     - :smile:

4. 实现算法的要点

   - 簇个数K的选择

   - 各个样本点到"簇中心"的==距离==

   - 根据新划分的簇,更新"簇中心"






